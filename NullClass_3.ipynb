{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27cbd7cb-b077-4af2-87a5-d08269240045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc3060c-12ff-4a6a-be0e-fbb79632b78e",
   "metadata": {},
   "source": [
    "# Indian Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9008018e-6a45-4c25-8473-0b381e5b95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Indian Sign Language Dataset (No predefined train/test split)\n",
    "# Assumes structure: dataset/Indian/A/, dataset/Indian/B/, ..., dataset/Indian/1/, ..., dataset/Indian/9/\n",
    "\n",
    "data_dir = r\"D:\\NULL CLASS\\Task 3\\Indian\"\n",
    "img_size = 64  # you can choose any suitable size\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for label in os.listdir(data_dir):\n",
    "    label_path = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_path):\n",
    "        for img_name in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, (img_size, img_size))\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67cb7d0-8820-42fe-87ce-9d5a6812fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are already inferred from folder names in the image loading step\n",
    "# But you can convert them to numeric labels using this mapping:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea12645-9d0f-4bc7-bcab-e39f8a01dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize is already done during image loading; now normalize the images\n",
    "images = images.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "192225a8-12f6-4f3b-a729-dce9a0bf144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(images, numeric_labels, test_size=0.2, random_state=42, stratify=numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61c9c04-1d91-46a1-b997-0134578c8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f2e795-afaa-4312-a935-23779ecdf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f117dd22-af8d-40a3-b040-754b7eb2cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca73bae-885b-4dea-b046-954badbf330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "350/350 [==============================] - 49s 136ms/step - loss: 0.5341 - accuracy: 0.8464 - val_loss: 0.0041 - val_accuracy: 0.9993\n",
      "Epoch 2/20\n",
      "350/350 [==============================] - 49s 140ms/step - loss: 0.0532 - accuracy: 0.9816 - val_loss: 0.0044 - val_accuracy: 0.9986\n",
      "Epoch 3/20\n",
      "350/350 [==============================] - 47s 134ms/step - loss: 0.0330 - accuracy: 0.9901 - val_loss: 4.1176e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "350/350 [==============================] - 48s 137ms/step - loss: 0.0228 - accuracy: 0.9932 - val_loss: 0.0010 - val_accuracy: 0.9993\n",
      "Epoch 5/20\n",
      "350/350 [==============================] - 48s 138ms/step - loss: 0.0193 - accuracy: 0.9937 - val_loss: 5.1570e-05 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "350/350 [==============================] - 49s 140ms/step - loss: 0.0191 - accuracy: 0.9932 - val_loss: 2.8907e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "350/350 [==============================] - 49s 140ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 8.6138e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "350/350 [==============================] - 50s 142ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0028 - val_accuracy: 0.9993\n",
      "Epoch 9/20\n",
      "350/350 [==============================] - 49s 139ms/step - loss: 0.0133 - accuracy: 0.9959 - val_loss: 1.1868e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "350/350 [==============================] - 49s 140ms/step - loss: 0.0064 - accuracy: 0.9974 - val_loss: 6.9684e-06 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "350/350 [==============================] - 48s 138ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 5.6551e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "350/350 [==============================] - 49s 141ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 3.4956e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "350/350 [==============================] - 49s 141ms/step - loss: 0.0081 - accuracy: 0.9971 - val_loss: 3.7986e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "350/350 [==============================] - 49s 139ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 7.5100e-04 - val_accuracy: 0.9996\n",
      "Epoch 15/20\n",
      "350/350 [==============================] - 49s 141ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 1.9034e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "350/350 [==============================] - 59s 169ms/step - loss: 0.0052 - accuracy: 0.9980 - val_loss: 6.6754e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "350/350 [==============================] - 59s 168ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 2.8722e-07 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "350/350 [==============================] - 51s 146ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 4.8573e-07 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "350/350 [==============================] - 51s 145ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 3.6486e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "350/350 [==============================] - 52s 149ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 2.2220e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b4aff5-1298-45ca-b701-f895a4bc2738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 25ms/step - loss: 2.2220e-04 - accuracy: 1.0000\n",
      "Validation Loss: 0.00022220012033358216\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2039aa7-0724-4a55-838a-f3df5ec383c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 2s 26ms/step - loss: 2.2220e-04 - accuracy: 1.0000\n",
      "Final Loss: 0.00022220012033358216\n",
      "Final Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on validation set (same as previous step)\n",
    "final_loss, final_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Final Loss: {final_loss}\")\n",
    "print(f\"Final Accuracy: {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f35624-0420-4c00-b3ef-05c713d1325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VISHWA\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"indian_sign_language_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f4b9ff4-6625-4c9d-9859-1090dd734939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def is_within_active_hours():\n",
    "    current_time = datetime.now().time()\n",
    "    start_time = datetime.strptime(\"18:00\", \"%H:%M\").time()\n",
    "    end_time = datetime.strptime(\"22:00\", \"%H:%M\").time()\n",
    "    return start_time <= current_time <= end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c39fd4b-f87b-4a51-a36a-94bdbbaa6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"indian_sign_language_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3acb3-ccc0-43b5-a0ca-f28ca922f457",
   "metadata": {},
   "source": [
    "GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae472d84-1f35-46d5-849b-3ef709c29d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, Label, Button\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "def upload_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        img = cv2.imread(file_path)\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = img.reshape(1, 64, 64, 3)\n",
    "        if is_within_active_hours():\n",
    "            prediction = model.predict(img)\n",
    "            predicted_label = label_encoder.inverse_transform([prediction.argmax()])[0]\n",
    "            result_label.config(text=f\"Prediction: {predicted_label}\")\n",
    "        else:\n",
    "            result_label.config(text=\"Access only allowed between 6 PM and 10 PM\")\n",
    "\n",
    "def start_video():\n",
    "    global cap\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    show_frame()\n",
    "\n",
    "def stop_video():\n",
    "    global cap\n",
    "    cap.release()\n",
    "    video_label.config(image='')\n",
    "\n",
    "def show_frame():\n",
    "    if cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            img = cv2.resize(frame, (64, 64))\n",
    "            img_normalized = img.astype('float32') / 255.0\n",
    "            img_normalized = img_normalized.reshape(1, 64, 64, 3)\n",
    "            if is_within_active_hours():\n",
    "                prediction = model.predict(img_normalized)\n",
    "                predicted_label = label_encoder.inverse_transform([prediction.argmax()])[0]\n",
    "                result_label.config(text=f\"Prediction: {predicted_label}\")\n",
    "            else:\n",
    "                result_label.config(text=\"Time restricted: 6 PM - 10 PM\")\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(frame)\n",
    "            imgtk = ImageTk.PhotoImage(image=img)\n",
    "            video_label.imgtk = imgtk\n",
    "            video_label.configure(image=imgtk)\n",
    "        video_label.after(10, show_frame)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Sign Language Detector\")\n",
    "\n",
    "upload_btn = Button(root, text=\"Upload Image\", command=upload_image)\n",
    "upload_btn.pack()\n",
    "\n",
    "start_btn = Button(root, text=\"Start Video\", command=start_video)\n",
    "start_btn.pack()\n",
    "\n",
    "stop_btn = Button(root, text=\"Stop Video\", command=stop_video)\n",
    "stop_btn.pack()\n",
    "\n",
    "video_label = Label(root)\n",
    "video_label.pack()\n",
    "\n",
    "result_label = Label(root, text=\"Prediction will appear here\")\n",
    "result_label.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
